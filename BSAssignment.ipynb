{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "91daf89c-a6fb-43ec-8f9d-3dec1dd6a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product 1:\n",
      "Name: bewakoof x dc\n",
      "Price: ₹499\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-adam-graphic-printed-t-shirt-541266-1709214736-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 2:\n",
      "Name: bewakoof x house of the dragon\n",
      "Price: ₹399\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-house-of-the-dragon-iconic-graphic-printed-t-shirt-519411-1715257899-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 3:\n",
      "Name: bewakoof x tom & jerry\n",
      "Price: ₹499\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-blue-moody-jerry-graphic-printed-oversized-t-shirt-585902-1715257595-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 4:\n",
      "Name: Bewakoof®\n",
      "Price: ₹439\n",
      "Image URL: https://images.bewakoof.com/t640/women-aop-oversize-t-shirt-12-580375-1684862463-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 5:\n",
      "Name: Bewakoof®\n",
      "Price: ₹499\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-warriors-graphic-printed-oversized-t-shirt-519149-1715257507-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 6:\n",
      "Name: bewakoof x peanuts\n",
      "Price: ₹599\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-orange-need-space-snoopy-graphic-printed-oversized-t-shirt-630653-1717579140-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 7:\n",
      "Name: Bewakoof®\n",
      "Price: ₹549\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-black-lost-mountains-graphic-printed-t-shirt-272010-1715257815-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 8:\n",
      "Name: Bewakoof®\n",
      "Price: ₹499\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-sun-kissed-green-t-shirt-315189-1679049039-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 9:\n",
      "Name: bewakoof x tom & jerry\n",
      "Price: ₹499\n",
      "Image URL: https://images.bewakoof.com/t640/women-s-green-weirdos-forever-graphic-printed-oversized-t-shirt-592030-1707292567-1.jpg\n",
      "--------------------------------------------------\n",
      "Product 10:\n",
      "Name: Bewakoof®\n",
      "Price: ₹499\n",
      "Image URL: https://images.bewakoof.com/t640/men-s-blue-rider-vroom-panda-graphic-printed-t-shirt-387282-1717060217-1.jpg\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.bewakoof.com/bestseller?sort=popular'\n",
    "\n",
    "# HTTP request to get HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# first 10 product details\n",
    "products = soup.find_all('div', class_='productCardBox', limit=10)\n",
    "\n",
    "product_details = []\n",
    "for product in products:\n",
    "    # product name\n",
    "    product_name = product.find('h3').get_text(strip=True)\n",
    "    \n",
    "    # product price\n",
    "    price = product.find('div', class_='discountedPriceText').get_text(strip=True)\n",
    "    \n",
    "    # product image URL\n",
    "    image_tag = product.find('img')\n",
    "    image_url = image_tag['src'] if image_tag else None\n",
    "    \n",
    "    product_details.append({\n",
    "        'Product Name': product_name,\n",
    "        'Price': price,\n",
    "        'Image URL': image_url\n",
    "    })\n",
    "\n",
    "# product details\n",
    "for idx, product in enumerate(product_details, 1):\n",
    "    print(f\"Product {idx}:\")\n",
    "    print(f\"Name: {product['Product Name']}\")\n",
    "    print(f\"Price: {product['Price']}\")\n",
    "    print(f\"Image URL: {product['Image URL']}\")\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1736d4d8-2b4a-49a5-94d8-e04e8817d500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the IMDb top 100 Indian movies list\n",
    "url = \"https://www.imdb.com/list/ls056092300/\"\n",
    "\n",
    "# HTTP request to get HTML content\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# first 10 movies\n",
    "movies = soup.find_all('div', class_='lister-item-content', limit=10)\n",
    "\n",
    "movie_details = []\n",
    "\n",
    "for movie in movies:\n",
    "    # movie title\n",
    "    title = movie.find('h3', class_='ipc-title__text')\n",
    "    title = title_tag.get_text(strip=True)\n",
    "    \n",
    "    # movie rating\n",
    "    rating_tag = movie.find('span', class_='ipc-rating-star--rating')\n",
    "    rating = rating_tag.get_text(strip=True) if rating_tag else \"N/A\"\n",
    "    \n",
    "    # movie release year\n",
    "    year_tag = movie.find('span', class_='sc-b189961a-8')\n",
    "    year = year_tag.get_text(strip=True).strip('()') if year_tag else \"N/A\"\n",
    "\n",
    "    movie_details.append({\n",
    "        'Title': title,\n",
    "        'Rating': rating,\n",
    "        'Year of Release': year\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(movie_details)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cdc8ebb5-3a7c-4fd3-ba96-54e8571cdaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded-articles/'\n",
    "\n",
    "# GET request to the website\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Parse the page content with BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all articles on the page\n",
    "    articles = soup.find_all('div', class_='article-details')\n",
    "\n",
    "    for article in articles:\n",
    "        # paper title\n",
    "        title_tag = article.find('h2', class_='article-title')\n",
    "        title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
    "        \n",
    "        # publication date \n",
    "        date_tag = article.find('p', class_='article-date')\n",
    "        date = date_tag.get_text(strip=True) if date_tag else 'N/A'\n",
    "        \n",
    "        # author names \n",
    "        authors_tag = article.find('p', class_='article-authors')\n",
    "        authors = authors_tag.get_text(strip=True) if authors_tag else 'N/A'\n",
    "\n",
    "        print(f\"Title: {title}\")\n",
    "        print(f\"Date: {date}\")\n",
    "        print(f\"Authors: {authors}\")\n",
    "        print('---')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2369af66-1211-4153-a6b0-68d7f4c1ff28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heading: MarketsBusinessInvestingTechPoliticsVideoWatchlistInvesting ClubPROLivestreamMenu\n",
      "Link: /\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.cnbc.com/world/?region=world'\n",
    "\n",
    "def scrape_cnbc_page(url):\n",
    "    try:\n",
    "        # GET request to the website\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all news articles\n",
    "        articles = soup.find_all('div', class_='CNBCGlobalNav-mobileNavMenu')\n",
    "\n",
    "        for article in articles:\n",
    "            # Extract heading\n",
    "            heading = article.get_text(strip=True)\n",
    "\n",
    "            # Extract news link\n",
    "            link_tag = article.find('a')\n",
    "            link = link_tag['href'] if link_tag else 'No link available'\n",
    "\n",
    "            print(f\"Heading: {heading}\")\n",
    "            print(f\"Link: {link}\")\n",
    "            # print(f\"Date: {date}\")\n",
    "            print('---')\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Other error occurred: {err}\")\n",
    "\n",
    "# Call the function\n",
    "scrape_cnbc_page(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "91f9bc41-0e41-465a-be02-8dabe3d75bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "def scrape_patreon_page(url):\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all posts \n",
    "        posts = soup.find_all('div', class_='sc-1cxu7e7-1 hrPBlE')  \n",
    "\n",
    "        for post in posts:\n",
    "            # Extract heading\n",
    "            heading = post.find('span', class_='sc-1cvoi1y-0 hxhWXn').get_text(strip=True)  \n",
    "\n",
    "            # Extract date\n",
    "            date = post.find('time').get_text(strip=True) if post.find('time') else 'Date not available'\n",
    "\n",
    "            # Extract content\n",
    "            content = post.find('v', class_='sc-1x8c58b-0 kdkysE').get_text(strip=True)  \n",
    "            \n",
    "            # Extract YouTube video link and likes\n",
    "            youtube_link = None\n",
    "            likes = 'Likes not available'\n",
    "            video_tags = post.find_all('iframe', src=True)\n",
    "            \n",
    "            for tag in video_tags:\n",
    "                src = tag['src']\n",
    "                if 'youtube.com' in src:\n",
    "                    youtube_link = src\n",
    "                    # Extract likes from the YouTube page\n",
    "                    video_id = re.search(r'v=([a-zA-Z0-9_-]+)', src).group(1)\n",
    "                    youtube_api_url = f'https://www.googleapis.com/youtube/v3/videos?part=statistics&id={video_id}&key=YOUR_API_KEY'\n",
    "                    video_response = requests.get(youtube_api_url)\n",
    "                    video_data = video_response.json()\n",
    "                    likes = video_data['items'][0]['statistics']['likeCount'] if 'items' in video_data and len(video_data['items']) > 0 else 'Likes not available'\n",
    "                    \n",
    "            print(f\"Heading: {heading}\")\n",
    "            print(f\"Date: {date}\")\n",
    "            print(f\"Content: {content}\")\n",
    "            print(f\"YouTube Link: {youtube_link}\")\n",
    "            print(f\"Likes: {likes}\")\n",
    "            print('---')\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"Other error occurred: {err}\")\n",
    "\n",
    "scrape_patreon_page(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8829b123-111b-4eb6-810e-7cfe7a3a2c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Location, Area, EMI, Price]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_house_details(locality):\n",
    "    \n",
    "    url = 'https://www.nobroker.in/'\n",
    "    \n",
    "    # Send a GET request to the website\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        titles = []\n",
    "        locations = []\n",
    "        areas = []\n",
    "        emis = []\n",
    "        prices = []\n",
    "\n",
    "        houses = soup.find_all('div', class_='card-header-title')\n",
    "        \n",
    "        for house in houses:\n",
    "            # house title\n",
    "            title = house.find('span', class_='overflow-hidden').get_text(strip=True)\n",
    "            titles.append(title)\n",
    "\n",
    "            # location \n",
    "            location_tag = house.find('div', class_='nb__1EwQz')\n",
    "            location = location_tag.get_text(strip=True) if location_tag else 'N/A'\n",
    "            locations.append(location)\n",
    "\n",
    "            # area \n",
    "            area_tag = house.find('div', class_='nb__2Miwj')\n",
    "            area = area_tag.get_text(strip=True).split('|')[1].strip() if area_tag else 'N/A'\n",
    "            areas.append(area)\n",
    "\n",
    "            # EMI \n",
    "            emi_tag = house.find('div', class_='nb__1vqA1')\n",
    "            emi = emi_tag.get_text(strip=True).split()[1] if emi_tag else 'N/A'\n",
    "            emis.append(emi)\n",
    "\n",
    "            # price\n",
    "            price_tag = house.find('div', class_='nb__1vZb2')\n",
    "            price = price_tag.get_text(strip=True) if price_tag else 'N/A'\n",
    "            prices.append(price)\n",
    "\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'Title': titles,\n",
    "            'Location': locations,\n",
    "            'Area': areas,\n",
    "            'EMI': emis,\n",
    "            'Price': prices\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {locality}. Status code: {response.status_code}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Localities to search\n",
    "localities = ['Indira Nagar', 'Jayanagar', 'Rajaji Nagar']\n",
    "\n",
    "# Combine results for all localities\n",
    "all_houses = pd.DataFrame()\n",
    "\n",
    "for locality in localities:\n",
    "    houses_df = fetch_house_details(locality)\n",
    "    all_houses = pd.concat([all_houses, houses_df], ignore_index=True)\n",
    "\n",
    "print(all_houses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448ef8d6-062a-4b89-b975-7074f93a460b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BSKERNEL",
   "language": "python",
   "name": "bskernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
